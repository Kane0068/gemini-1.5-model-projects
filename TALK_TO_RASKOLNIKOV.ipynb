{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 83735,
          "databundleVersionId": 9881586,
          "sourceType": "competition"
        },
        {
          "sourceId": 9696755,
          "sourceType": "datasetVersion",
          "datasetId": 5929029
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "TALK TO RASKOLNIKOV",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "vo1uPPty-fAI"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "gemini_long_context_path = kagglehub.competition_download('gemini-long-context')\n",
        "kane0068_crime_and_punishment_english_path = kagglehub.dataset_download('kane0068/crime-and-punishment-english')\n",
        "google_gemini_1_5_pro_api_api_gemini_1_5_pro_1_path = kagglehub.model_download('google/gemini-1.5-pro-api/Api/gemini-1.5-pro/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "xIftlD9j-fAM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interview with Rodion Romanovich Raskolnikov, the hero of Fyodor Dostoyevsky's novel Crime and Punishment**"
      ],
      "metadata": {
        "id": "yUGc24_q-fAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import logging\n",
        "from collections import deque\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from datetime import datetime\n",
        "# API\n",
        "from kaggle_secrets import UserSecretsClient"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T20:59:57.595639Z",
          "iopub.execute_input": "2024-11-14T20:59:57.596326Z",
          "iopub.status.idle": "2024-11-14T20:59:57.604479Z",
          "shell.execute_reply.started": "2024-11-14T20:59:57.596265Z",
          "shell.execute_reply": "2024-11-14T20:59:57.602868Z"
        },
        "id": "MFQbEYlp-fAO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenCounter:\n",
        "    def __init__(self):\n",
        "        self.total_prompt_tokens = 0\n",
        "        self.total_response_tokens = 0\n",
        "        self.context_window_sizes = []  # Track context window usage\n",
        "\n",
        "    def estimate_tokens(self, text: str) -> int:\n",
        "        #Approximate token count estimation\n",
        "        #     Rough estimation: average English word is ~1.3 tokens\n",
        "        return len(text.split()) * 1.3\n",
        "\n",
        "    def update_counts(self, prompt: str, response: str):\n",
        "        prompt_tokens = self.estimate_tokens(prompt)\n",
        "        response_tokens = self.estimate_tokens(response)\n",
        "\n",
        "        self.total_prompt_tokens += prompt_tokens\n",
        "        self.total_response_tokens += response_tokens\n",
        "\n",
        "        # Track context window size for this interaction\n",
        "        self.context_window_sizes.append(prompt_tokens + response_tokens)\n",
        "\n",
        "    def get_total_tokens(self) -> int:\n",
        "        return int(self.total_prompt_tokens + self.total_response_tokens)\n",
        "\n",
        "    def get_stats(self) -> dict:\n",
        "        stats = {\n",
        "            \"prompt_tokens\": int(self.total_prompt_tokens),\n",
        "            \"response_tokens\": int(self.total_response_tokens),\n",
        "            \"total_tokens\": self.get_total_tokens(),\n",
        "            \"max_window_size\": int(max(self.context_window_sizes)) if self.context_window_sizes else 0,\n",
        "            \"avg_window_size\": int(sum(self.context_window_sizes) / len(self.context_window_sizes)) if self.context_window_sizes else 0\n",
        "        }\n",
        "        return stats"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T20:59:57.607069Z",
          "iopub.execute_input": "2024-11-14T20:59:57.607584Z",
          "iopub.status.idle": "2024-11-14T20:59:57.624577Z",
          "shell.execute_reply.started": "2024-11-14T20:59:57.607529Z",
          "shell.execute_reply": "2024-11-14T20:59:57.623174Z"
        },
        "id": "7ALxCRmy-fAP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class TextLoader:\n",
        "    def __init__(self, file_path: str):\n",
        "        self.file_path = file_path\n",
        "        self.content = \"\"\n",
        "        self.chunks = []\n",
        "        self.chunk_size = 5000  # Can be increased due to Gemini's large context\n",
        "        self.token_counter = TokenCounter()\n",
        "        self.chunk_summaries = []  # Store summaries for each chunk\n",
        "\n",
        "    def load_and_chunk_text(self):\n",
        "        try:\n",
        "            with open(self.file_path, 'r', encoding='utf-8') as file:\n",
        "                self.content = file.read()\n",
        "\n",
        "            # Update token count\n",
        "            self.token_counter.update_counts(self.content, \"\")\n",
        "\n",
        "            # Chunk the text\n",
        "            self.chunks = []\n",
        "            current_chunk = \"\"\n",
        "\n",
        "            paragraphs = self.content.split('\\n\\n')\n",
        "            for paragraph in paragraphs:\n",
        "                if len(current_chunk) + len(paragraph) < self.chunk_size:\n",
        "                    current_chunk += paragraph + '\\n\\n'\n",
        "                else:\n",
        "                    self.chunks.append(current_chunk)\n",
        "                    current_chunk = paragraph + '\\n\\n'\n",
        "\n",
        "            if current_chunk:\n",
        "                self.chunks.append(current_chunk)\n",
        "\n",
        "            return len(self.chunks)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading text: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T20:59:57.626381Z",
          "iopub.execute_input": "2024-11-14T20:59:57.626962Z",
          "iopub.status.idle": "2024-11-14T20:59:57.640727Z",
          "shell.execute_reply.started": "2024-11-14T20:59:57.626905Z",
          "shell.execute_reply": "2024-11-14T20:59:57.63908Z"
        },
        "id": "cuLUGJvA-fAQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class RaskolnikovAI:\n",
        "    def __init__(self, api_key: str, book_path: str):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
        "        self.loader = TextLoader(book_path)\n",
        "        self.context = \"\"\n",
        "        self.training_done = False\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.token_counter = TokenCounter()\n",
        "        self.conversation_memory = deque(maxlen=5)  # Store recent conversations\n",
        "        self.analysis_results = []  # Store analysis results from training\n",
        "\n",
        "    def train_with_book(self):\n",
        "        try:\n",
        "            num_chunks = self.loader.load_and_chunk_text()\n",
        "            print(f\"Book loaded successfully. Processing {num_chunks} sections...\")\n",
        "\n",
        "            context_prompt = \"\"\"\n",
        "            Analyze the following text from Crime and Punishment to deeply understand\n",
        "            Raskolnikov's character, his psychological state, and key events.\n",
        "            Focus on:\n",
        "            1. His motivations and philosophical ideas\n",
        "            2. His psychological state and emotional journey\n",
        "            3. Key interactions with other characters\n",
        "            4. Important events and their impact on him\n",
        "\n",
        "            Text chunks will follow. Build a comprehensive understanding of the character.\n",
        "            \"\"\"\n",
        "\n",
        "            for i, chunk in enumerate(self.loader.chunks):\n",
        "                try:\n",
        "                    prompt = f\"{context_prompt}\\n\\nText chunk {i+1}:\\n{chunk}\"\n",
        "                    response = self.model.generate_content(prompt)\n",
        "\n",
        "                    # Store analysis results\n",
        "                    self.analysis_results.append(response.text)\n",
        "\n",
        "                    # Update token counts\n",
        "                    self.token_counter.update_counts(prompt, response.text)\n",
        "\n",
        "                    print(f\"Processed chunk {i+1}/{num_chunks}\")\n",
        "                    print(f\"Accumulated tokens: {self.token_counter.get_total_tokens():,}\")\n",
        "\n",
        "                    # Save progress periodically\n",
        "                    if (i + 1) % 5 == 0:\n",
        "                        self._save_progress()\n",
        "\n",
        "                    time.sleep(2)\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing chunk {i+1}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            self.training_done = True\n",
        "            self._save_final_analysis()\n",
        "\n",
        "            token_stats = self.token_counter.get_stats()\n",
        "            print(\"\\nProcessing completed!\")\n",
        "            print(f\"Total prompt tokens: {token_stats['prompt_tokens']:,}\")\n",
        "            print(f\"Total response tokens: {token_stats['response_tokens']:,}\")\n",
        "            print(f\"Total tokens processed: {token_stats['total_tokens']:,}\")\n",
        "            print(f\"Maximum context window size: {token_stats['max_window_size']:,}\")\n",
        "            print(f\"Average context window size: {token_stats['avg_window_size']:,}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Training error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def generate_response(self, user_input: str) -> str:\n",
        "        if not self.training_done:\n",
        "            return \"Please complete the book processing first.\"\n",
        "\n",
        "        try:\n",
        "            # Include recent conversation history\n",
        "            recent_history = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in self.conversation_memory])\n",
        "\n",
        "            character_prompt = f\"\"\"\n",
        "            You are Rodion Raskolnikov from Crime and Punishment. Respond as him,\n",
        "            incorporating his complex psychological state, philosophical beliefs,\n",
        "            and personal experiences from the novel. Consider:\n",
        "\n",
        "            1. Your theory of the extraordinary man\n",
        "            2. Your feelings about the murder and guilt\n",
        "            3. Your relationship with Sonia and other characters\n",
        "            4. Your current psychological and emotional state\n",
        "\n",
        "            Recent conversation history:\n",
        "            {recent_history}\n",
        "\n",
        "            Maintain Raskolnikov's intellectual depth and internal conflicts in your response.\n",
        "            \"\"\"\n",
        "\n",
        "            prompt = f\"{character_prompt}\\n\\nQuestion: {user_input}\\n\\nRespond as Raskolnikov:\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "            response_text = response.text\n",
        "\n",
        "            # Update conversation memory\n",
        "            self.conversation_memory.append((user_input, response_text))\n",
        "\n",
        "            # Update token counts\n",
        "            self.token_counter.update_counts(prompt, response_text)\n",
        "\n",
        "            # Log current token usage\n",
        "            print(f\"\\nCurrent total tokens: {self.token_counter.get_total_tokens():,}\")\n",
        "\n",
        "            return response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating response: {str(e)}\")\n",
        "            return \"I must gather my thoughts... (An error occurred)\"\n",
        "\n",
        "    def _save_progress(self):\n",
        "        \"\"\"Save training progress and analysis results\"\"\"\n",
        "        try:\n",
        "            progress_data = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'token_stats': self.token_counter.get_stats(),\n",
        "                'analysis_results': self.analysis_results\n",
        "            }\n",
        "\n",
        "            with open('training_progress.json', 'w', encoding='utf-8') as f:\n",
        "                json.dump(progress_data, f, indent=2)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving progress: {str(e)}\")\n",
        "\n",
        "    def _save_final_analysis(self):\n",
        "        \"\"\"Save final analysis results and statistics\"\"\"\n",
        "        try:\n",
        "            final_data = {\n",
        "                'completion_time': datetime.now().isoformat(),\n",
        "                'token_stats': self.token_counter.get_stats(),\n",
        "                'analysis_results': self.analysis_results,\n",
        "                'training_summary': self._generate_training_summary()\n",
        "            }\n",
        "\n",
        "            with open('final_analysis.json', 'w', encoding='utf-8') as f:\n",
        "                json.dump(final_data, f, indent=2)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving final analysis: {str(e)}\")\n",
        "\n",
        "\n",
        "    def _generate_training_summary(self) -> dict:\n",
        "        \"\"\"Generate summary of training process and results\"\"\"\n",
        "        return {\n",
        "            'total_chunks': len(self.loader.chunks),\n",
        "            'token_usage': self.token_counter.get_stats(),\n",
        "            'training_duration': str(datetime.now())\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T20:59:57.731086Z",
          "iopub.execute_input": "2024-11-14T20:59:57.731566Z",
          "iopub.status.idle": "2024-11-14T20:59:57.760304Z",
          "shell.execute_reply.started": "2024-11-14T20:59:57.731521Z",
          "shell.execute_reply": "2024-11-14T20:59:57.758606Z"
        },
        "id": "Jbf-bkN_-fAQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "    user_secrets = UserSecretsClient()\n",
        "    API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
        "    BOOK_PATH = \"/kaggle/input/crime-and-punishment-english/crime_and_punishment_english.txt\"\n",
        "\n",
        "    try:\n",
        "        # Initialize and train the model\n",
        "        raskolnikov = RaskolnikovAI(API_KEY, BOOK_PATH)\n",
        "        print(\"Processing Crime and Punishment...\")\n",
        "        raskolnikov.train_with_book()\n",
        "\n",
        "        print(\"\\nYou can now converse with Raskolnikov. Type 'quit' to exit.\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"\\nYour question: \").strip()\n",
        "            if user_input.lower() == 'quit':\n",
        "                # Display final statistics\n",
        "                token_stats = raskolnikov.token_counter.get_stats()\n",
        "                print(\"\\nFinal Token Statistics:\")\n",
        "                print(f\"Total prompt tokens: {token_stats['prompt_tokens']:,}\")\n",
        "                print(f\"Total response tokens: {token_stats['response_tokens']:,}\")\n",
        "                print(f\"Total tokens processed: {token_stats['total_tokens']:,}\")\n",
        "                print(f\"Maximum context window: {token_stats['max_window_size']:,}\")\n",
        "                print(f\"Average context window: {token_stats['avg_window_size']:,}\")\n",
        "                print(f\"Window utilization: {(token_stats['max_window_size'] / 2000000 * 100):.2f}%\")\n",
        "\n",
        "                # Print usage graph\n",
        "                width = 40\n",
        "                used = int((token_stats['max_window_size'] / 2000000) * width)\n",
        "                print(\"\\nWindow Usage: [\", end=\"\")\n",
        "                print(\"=\" * used + \" \" * (width - used) + \"]\")\n",
        "                print(f\"{'0':8} {token_stats['max_window_size']:,}{'2M':>8}\")\n",
        "\n",
        "                break\n",
        "\n",
        "            response = raskolnikov.generate_response(user_input)\n",
        "            print(f\"\\nRaskolnikov: {response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        logging.error(f\"System error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T20:59:57.762664Z",
          "iopub.execute_input": "2024-11-14T20:59:57.76323Z",
          "iopub.status.idle": "2024-11-14T21:02:17.637222Z",
          "shell.execute_reply.started": "2024-11-14T20:59:57.763163Z",
          "shell.execute_reply": "2024-11-14T21:02:17.635596Z"
        },
        "id": "60XW-MsX-fAR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Questions :\n",
        "\n",
        "- Why did you believe that your crime could be justified as a moral act?\n",
        "- Do you see yourself as a “superman” above ordinary morality, or was that simply a way to rationalize your actions?\n",
        "- After committing the murder, did you feel any sense of relief, or did guilt consume you immediately?\n",
        "- What were you hoping to accomplish by killing the pawnbroker, and do you think it was worth the price you paid emotionally?\n",
        "- How did your interactions with Sonia influence your view on redemption and morality?\n",
        "- Do you regret your actions, or do you still believe in your theory that some individuals have the right to bypass moral constraints?\n",
        "- How did the suffering of those around you, like your mother and sister, affect your own perception of guilt and responsibility?\n",
        "- What role did poverty and desperation play in pushing you towards committing the crime?\n",
        "- How would you describe your relationship with Inspector Porfiry, and do you feel he understood your true nature?\n",
        "- Did you find any meaning or personal growth through the hardships you endured, or do you see it all as needless suffering?"
      ],
      "metadata": {
        "id": "LY5JQp3s-fAS"
      }
    }
  ]
}