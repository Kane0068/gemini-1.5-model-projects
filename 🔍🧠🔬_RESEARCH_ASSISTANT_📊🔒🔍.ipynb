{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 83735,
          "databundleVersionId": 9881586,
          "sourceType": "competition"
        },
        {
          "sourceId": 9887278,
          "sourceType": "datasetVersion",
          "datasetId": 6071868
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "ðŸ”ðŸ§ ðŸ”¬ RESEARCH_ASSISTANT ðŸ“ŠðŸ”’ðŸ”",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "wthNH75E_zOU"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "gemini_long_context_path = kagglehub.competition_download('gemini-long-context')\n",
        "kane0068_pdf_files_path = kagglehub.dataset_download('kane0068/pdf-files')\n",
        "google_gemini_1_5_flash_api_api_gemini_1_5_flash_1_path = kagglehub.model_download('google/gemini-1.5-flash-api/Api/gemini-1.5-flash/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "j8FKVW6Z_zOc"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code represents a sophisticated Research Assistant class designed for comprehensive academic paper analysis using Google's Gemini AI. Let me break down its most impressive features:\n",
        "\n",
        "# ðŸ”¬ Advanced Paper Processing Capabilities:\n",
        "\n",
        "Extracts detailed information from academic PDFs\n",
        "Parses metadata, abstracts, and full content\n",
        "Automatic keyword and reference extraction\n",
        "Robust error handling and logging\n",
        "\n",
        "# ðŸ§  AI-Powered Analysis Features:\n",
        "\n",
        "Uses Gemini AI to identify:\n",
        "\n",
        "Connections between papers\n",
        "Research gaps\n",
        "Future research directions\n",
        "Key findings\n",
        "\n",
        "\n",
        "Advanced text similarity calculations\n",
        "Topic clustering\n",
        "Citation network analysis\n",
        "\n",
        "# ðŸ“Š Visualization and Reporting:\n",
        "\n",
        "Generates comprehensive markdown reports\n",
        "Creates multiple visualizations:\n",
        "\n",
        "Paper similarity heatmaps\n",
        "Interactive citation network graphs\n",
        "Topic distribution charts\n",
        "Methodology distribution pie charts\n",
        "\n",
        "\n",
        "\n",
        "# ðŸ”’ Smart Rate Limiting:\n",
        "\n",
        "Implements request rate limiting for Gemini API\n",
        "Prevents overwhelming the API with too many requests\n",
        "\n",
        "# ðŸ” Key Technical Highlights:\n",
        "\n",
        "Uses advanced NLP techniques like TF-IDF vectorization\n",
        "Implements network analysis with NetworkX\n",
        "Supports batch processing of multiple research papers\n",
        "Flexible configuration for cache directories and logging\n",
        "\n",
        "In essence, this is a powerful, AI-enhanced research assistant that can automatically analyze, connect, and visualize insights from academic literature, making complex research synthesis much more efficient and insightful."
      ],
      "metadata": {
        "id": "mHIj8nGb_zOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2 typing"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-14T18:20:03.744021Z",
          "iopub.execute_input": "2024-11-14T18:20:03.744517Z",
          "iopub.status.idle": "2024-11-14T18:20:18.10327Z",
          "shell.execute_reply.started": "2024-11-14T18:20:03.744465Z",
          "shell.execute_reply": "2024-11-14T18:20:18.101827Z"
        },
        "trusted": true,
        "id": "3BaN4Ny0_zOh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import os\n",
        "import json\n",
        "import PyPDF2\n",
        "import google.generativeai as genai\n",
        "from typing import List, Dict, Any, Optional\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import re\n",
        "from functools import wraps\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from functools import lru_cache\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Interactive plotting\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-14T18:20:18.105818Z",
          "iopub.execute_input": "2024-11-14T18:20:18.106255Z",
          "iopub.status.idle": "2024-11-14T18:20:21.687818Z",
          "shell.execute_reply.started": "2024-11-14T18:20:18.106211Z",
          "shell.execute_reply": "2024-11-14T18:20:21.686591Z"
        },
        "trusted": true,
        "id": "UZrXF6i-_zOi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DataTypes\n",
        "@dataclass\n",
        "class Paper:\n",
        "    title : str\n",
        "    authors : List[str]\n",
        "    year : int\n",
        "    abstract :str\n",
        "    content : str\n",
        "    filepath : str\n",
        "    references : List[str] = None\n",
        "    citations : List[str] = None\n",
        "    keywords : List[str] = None\n",
        "\n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "@dataclass\n",
        "class Analysis:\n",
        "    connections : List[Dict]\n",
        "    research_gaps : List[str]\n",
        "    future_directions : List[str]\n",
        "    key_findings : List[str]\n",
        "    methodology_analysis : Dict\n",
        "    topic_clusters :Dict\n",
        "    citation_network : Dict\n",
        "    similarity_scores : Dict"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-14T18:20:21.689366Z",
          "iopub.execute_input": "2024-11-14T18:20:21.69027Z",
          "iopub.status.idle": "2024-11-14T18:20:21.70251Z",
          "shell.execute_reply.started": "2024-11-14T18:20:21.69021Z",
          "shell.execute_reply": "2024-11-14T18:20:21.700868Z"
        },
        "trusted": true,
        "id": "DFi1RvYu_zOj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#For Gemini Request Control\n",
        "\n",
        "class RateLimiter:\n",
        "    def __init__(self, max_requests, time_period):\n",
        "        self.max_requests = max_requests\n",
        "        self.time_period = time_period\n",
        "        self.request_times = []\n",
        "\n",
        "    def __call__(self, func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            now = time.time()\n",
        "            self.request_times = [t for t in self.request_times if t > now - self.time_period]\n",
        "\n",
        "            if len(self.request_times) >= self.max_requests:\n",
        "                wait_time = self.time_period - (now - self.request_times[0])\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            self.request_times.append(now)\n",
        "            return func(*args, **kwargs)\n",
        "        return wrapper\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResearchAssistant:\n",
        "    def __init__(self,api_key : str , cache_dir :str = \"./cache\"):\n",
        "        #Start Research Assistant with Google API key and set up logging\n",
        "\n",
        "        self.setup_logging()\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "         # Cache Config\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok = True)\n",
        "\n",
        "        # Google Api Config\n",
        "        genai.configure(api_key = api_key)\n",
        "        self.model = genai.GenerativeModel(model_name='gemini-1.5-flash-latest') #\n",
        "        self.chat = self.model.start_chat()\n",
        "\n",
        "\n",
        "        # Analysis Tools\n",
        "#         self.vectorizer = TfidfVectorizer(stop_words = 'english')\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            stop_words='english',\n",
        "            min_df=1,  # Include terms that appear in at least 1 document\n",
        "            max_df=0.95,  # Exclude terms that appear in more than 95% of documents\n",
        "            token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character\n",
        "            strip_accents='unicode'\n",
        "        )\n",
        "        self.tokenizer = genai.GenerativeModel('gemini-1.5-flash-latest').count_tokens\n",
        "        self.logger.info('Research Assistant initialized successfully')\n",
        "\n",
        "\n",
        "    def count_tokens_in_text(self, text: str) -> int:\n",
        "        ##Count tokens in given text using Gemini's tokenizer\n",
        "        try:\n",
        "            result = self.tokenizer(text)\n",
        "            return result.total_tokens\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error counting tokens: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def setup_logging(self):\n",
        "\n",
        "        logging.basicConfig(\n",
        "            level=logging.DEBUG,\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('research_assistant_debug.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def extract_text_from_pdf(self,filepath : str) -> Optional[Dict[str, str]]:\n",
        "\n",
        "        try:\n",
        "            self.logger.debug(f\"Starting PDF extraction from {filepath}\")\n",
        "\n",
        "            if not os.path.exists(filepath):\n",
        "                self.logger.error(f'File Not Found : {filepath}')\n",
        "                return None\n",
        "\n",
        "            with open(filepath,'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                if not pdf_reader.pages:\n",
        "                    self.logger.error(\"PDF has no page\")\n",
        "                    return None\n",
        "\n",
        "                # Extract Metadata\n",
        "                metadata = {}\n",
        "\n",
        "                if pdf_reader.metadata:\n",
        "                    metadata = {\n",
        "                        'title' : pdf_reader.metadata.get('/Title',''),\n",
        "                        'author': pdf_reader.metadata.get('/Author',''),\n",
        "                        'subject': pdf_reader.metadata.get('/Subject',''),\n",
        "                        'keywords' : pdf_reader.metadata.get('/Keywords','')\n",
        "                    }\n",
        "                else:\n",
        "                    self.logger.warning('No metadata found in PDF')\n",
        "                # EXtract Text\n",
        "                text = \"\"\n",
        "                abstract = \"\"\n",
        "\n",
        "                for i ,page in enumerate(pdf_reader.pages):\n",
        "                    try:\n",
        "                        content = page.extract_text()\n",
        "                        self.logger.debug(f\"Page {i+1} extracted,length :{len(content)}\")\n",
        "                        text += content\n",
        "\n",
        "                    #Abstract from the first page\n",
        "                        if i == 0:\n",
        "                            abstract_match = re.search(r'Abstract\\s*(.*?)(?=\\n\\n|\\n[A-Z]{2,})', content, re.DOTALL)\n",
        "                            if abstract_match:\n",
        "                                abstract = abstract_match.group(1).strip()\n",
        "                                self.logger.debug(f\"Abstract found , length :{len(abstract)}\")\n",
        "\n",
        "                            else:\n",
        "                                self.logger.warning('No abstract pattern found on first page')\n",
        "                    except Exception as e:\n",
        "                        self.logger.error(f\"Error extracting text from page {i+1}: {str(e)}\")\n",
        "                result =  {\n",
        "                    'metadata' :metadata,\n",
        "                    'abstract' : abstract,\n",
        "                    'content' : text\n",
        "                }\n",
        "\n",
        "                self.logger.info(f\"PDF extraction completed. \"\n",
        "                               f\"Metadata keys: {list(metadata.keys())}, \"\n",
        "                               f\"Abstract length: {len(abstract)}, \"\n",
        "                               f\"Content length: {len(text)}\")\n",
        "\n",
        "                return result\n",
        "\n",
        "        except PyPDF2.PdfReadError as e:\n",
        "            self.logger.error(f\"PDF reading error: {str(e)}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Unexpected error in PDF extraction: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def test_gemini_connection(self):\n",
        "        try:\n",
        "            response = self.model.generate_content(\"Please respond with 'Connection successful' if you receive this message.\")\n",
        "            print(f\"Gemini response: {response.text}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Gemini connection failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def parse_paper(self, filepath: str) -> Optional[Paper]:\n",
        "        try:\n",
        "            self.logger.info(f\"Starting to parse paper: {filepath}\")\n",
        "\n",
        "            extracted_data = self.extract_text_from_pdf(filepath)\n",
        "            if not extracted_data:\n",
        "                self.logger.error(\"No data extracted from PDF\")\n",
        "                return None\n",
        "\n",
        "            # Log the extracted data\n",
        "            self.logger.debug(f\"Extracted metadata: {extracted_data['metadata']}\")\n",
        "            self.logger.debug(f\"Abstract length: {len(extracted_data['abstract'])}\")\n",
        "            self.logger.debug(f\"Content length: {len(extracted_data['content'])}\")\n",
        "\n",
        "            # Validate and clean metadata\n",
        "            title = extracted_data['metadata'].get('title', '').strip()\n",
        "            authors = [author.strip() for author in extracted_data['metadata'].get('author', '').split(',') if author.strip()]\n",
        "            subject = extracted_data['metadata'].get('subject', '').strip()\n",
        "\n",
        "            # Enhanced year extraction\n",
        "            year = None\n",
        "            year_patterns = [\n",
        "                r\"20[0-2][0-9]\",  # Standard year format\n",
        "                r\"Â©\\s*20[0-2][0-9]\",  # Copyright year\n",
        "                r\"Published.*20[0-2][0-9]\",  # Publication year\n",
        "            ]\n",
        "\n",
        "            for pattern in year_patterns:\n",
        "                year_match = re.search(pattern, subject or extracted_data['content'])\n",
        "                if year_match:\n",
        "                    year_str = re.search(r\"20[0-2][0-9]\", year_match.group()).group()\n",
        "                    year = int(year_str)\n",
        "                    self.logger.debug(f\"Year found: {year} using pattern: {pattern}\")\n",
        "                    break\n",
        "\n",
        "            if not year:\n",
        "                self.logger.warning(\"No year found in document\")\n",
        "\n",
        "            # Log validation results\n",
        "            self.logger.info(f\"Parsed data - Title: {bool(title)}, \"\n",
        "                           f\"Authors: {len(authors)}, \"\n",
        "                           f\"Year: {year}\")\n",
        "\n",
        "            return Paper(\n",
        "                title=title or \"Untitled\",\n",
        "                authors=authors or [\"Unknown\"],\n",
        "                year=year,\n",
        "                abstract=extracted_data['abstract'],\n",
        "                content=extracted_data['content'],\n",
        "                filepath=filepath,\n",
        "                references=None,\n",
        "                keywords=None\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error parsing paper: {str(e)}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def extract_references(self,content : str) -> List[str]:\n",
        "        # can be improved...\n",
        "        references = []\n",
        "\n",
        "        ref_section = re.search(r\"References\\s*(.*?)(?=\\n\\n|\\Z)\" , content , re.DOTALL)\n",
        "        if ref_section :\n",
        "            ref_text = ref_section.group(1)\n",
        "\n",
        "            references = [ref.strip() for ref in ref_text.split('\\n') if ref.strip()]\n",
        "\n",
        "        return references\n",
        "\n",
        "    def extract_keywords(self, content: str) -> List[str]:\n",
        "\n",
        "        keywords = []\n",
        "        try:\n",
        "        # Look for keywords with different possible formats\n",
        "            patterns = [\n",
        "            r'Keywords:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "            r'Key\\s+words:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "            r'Index\\s+Terms:?\\s*(.*?)(?=\\n\\n|\\Z)'\n",
        "            ]\n",
        "\n",
        "            for pattern in patterns:\n",
        "                keyword_section = re.search(pattern, content, re.DOTALL | re.IGNORECASE)\n",
        "                if keyword_section:\n",
        "                # Split by common separators and clean\n",
        "                    raw_keywords = re.split(r'[;,]', keyword_section.group(1))\n",
        "                    keywords = [k.strip() for k in raw_keywords if k.strip()]\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting keywords: {str(e)}\")\n",
        "\n",
        "        return keywords\n",
        "\n",
        "\n",
        "\n",
        "    def analyze_papers(self, papers: List[Paper]) -> Analysis:\n",
        "        try:\n",
        "            # Count tokens before analysis\n",
        "            total_tokens = 0\n",
        "            token_counts = {}\n",
        "\n",
        "            for paper in papers:\n",
        "                # Combine all text content for token counting\n",
        "                paper_text = f\"\"\"\n",
        "                Title: {paper.title}\n",
        "                Abstract: {paper.abstract}\n",
        "                Authors: {', '.join(paper.authors)}\n",
        "                Year: {paper.year}\n",
        "                Keywords: {', '.join(paper.keywords or [])}\n",
        "                Content: {paper.content}\n",
        "                \"\"\"\n",
        "\n",
        "                tokens = self.count_tokens_in_text(paper_text)\n",
        "                token_counts[paper.title] = tokens\n",
        "                total_tokens += tokens\n",
        "\n",
        "            self.logger.info(f\"Total tokens processed: {total_tokens}\")\n",
        "            self.logger.info(\"Token counts per paper:\")\n",
        "            for title, count in token_counts.items():\n",
        "                self.logger.info(f\"- {title}: {count} tokens\")\n",
        "\n",
        "            # Existing analysis code\n",
        "            gemini_analysis = self._analyze_with_gemini(papers)\n",
        "            similarity_scores = self._calculate_similarity_scores(papers)\n",
        "            topic_clusters = self._cluster_topics(papers)\n",
        "            citation_network = self._analyze_citation_network(papers)\n",
        "            methodology_analysis = self._analyze_methodologies(papers)\n",
        "\n",
        "            analysis = Analysis(\n",
        "                connections = gemini_analysis['connections'],\n",
        "                research_gaps = gemini_analysis['research_gaps'],\n",
        "                future_directions = gemini_analysis['future_directions'],\n",
        "                key_findings = gemini_analysis['key_findings'],\n",
        "                methodology_analysis = methodology_analysis,\n",
        "                topic_clusters = topic_clusters,\n",
        "                citation_network = citation_network,\n",
        "                similarity_scores = similarity_scores\n",
        "            )\n",
        "\n",
        "            # Add token counts to the analysis\n",
        "            analysis.token_counts = token_counts\n",
        "            analysis.total_tokens = total_tokens\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in Papers Analysis: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    #Per minute: Gemini 1.5 flash ~10-15 requests - Gemini 1.5 Pro ~2 requests - Gemini 1.5 8B ~10-15\n",
        "    @RateLimiter(max_requests=15, time_period=60)\n",
        "    def _analyze_with_gemini(self, papers: List[Paper]) -> Dict:\n",
        "        try:\n",
        "            self.logger.info(\"Attempting to connect to Gemini...\")\n",
        "            prompt = self._construct_analysis_prompt(papers)\n",
        "            self.logger.info(\"Sending prompt to Gemini...\")\n",
        "            response = self.chat.send_message(prompt)\n",
        "            self.logger.info(f\"Received response from Gemini: {response.text[:100]}...\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Gemini API error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        analysis = {\n",
        "        'connections': [],\n",
        "        'research_gaps': [],\n",
        "        'future_directions': [],\n",
        "        'key_findings': []\n",
        "        }\n",
        "\n",
        "    # For more flexible header matching\n",
        "        current_section = None\n",
        "        for line in response.text.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Control titles more flexibly\n",
        "            lower_line = line.lower()\n",
        "            if \"key findings\" in lower_line:\n",
        "                current_section = 'key_findings'\n",
        "                continue\n",
        "            elif \"research gaps\" in lower_line:\n",
        "                current_section = 'research_gaps'\n",
        "                continue\n",
        "            elif \"future\" in lower_line and \"direction\" in lower_line:\n",
        "                current_section = 'future_directions'\n",
        "                continue\n",
        "            elif \"connection\" in lower_line:\n",
        "                current_section = 'connections'\n",
        "                continue\n",
        "\n",
        "            # # Clear bullets-\n",
        "                line = line.lstrip('* ')\n",
        "            if line.startswith('-'):\n",
        "                line = line.lstrip('- ')\n",
        "\n",
        "            # Add content to relevant section\n",
        "            if current_section and line:\n",
        "                if current_section == 'connections':\n",
        "                    connection = {\n",
        "                        'papers': [p.title for p in papers],\n",
        "                        'description': line,\n",
        "                        'strength': 0.8\n",
        "                    }\n",
        "                    analysis[current_section].append(connection)\n",
        "                else:\n",
        "                    # Clear title marks\n",
        "                    line = line.replace('**', '')\n",
        "                    if line and not line.endswith(':'):  # BaÅŸlÄ±k satÄ±rlarÄ±nÄ± atla\n",
        "                        analysis[current_section].append(line)\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _construct_analysis_prompt(self,papers : List[Paper]) -> str:\n",
        "        # Creating an analysis prompts for the Gemini -            !!!Can be Ä°mproved\n",
        "        prompt = \"\"\"\n",
        "        Analyze for the following academic papers and provide:\n",
        "        1.Connections between papers(including methodological and theoretical links)\n",
        "        2.Research gaps in the field\n",
        "        3.Promising future research directions\n",
        "        4.Key findings and their implications\n",
        "\n",
        "        Papers to analyze:\n",
        "        \"\"\"\n",
        "\n",
        "        for paper in papers:\n",
        "            prompt += f\"\\nTitle: {paper.title}\\n\"\n",
        "            prompt += f\"Abstract: {paper.abstract}\\n\"\n",
        "            prompt += f\"Authors: {', '.join(paper.authors)}\\n\"\n",
        "            prompt += f\"Year: {paper.year}\\n\"\n",
        "            prompt += f\"Keywords: {', '.join(paper.keywords or [])}\\n\"\n",
        "            prompt += f\"Content excerpt: {paper.content[:2000]}...\\n\\n\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "\n",
        "\n",
        "    def _calculate_similarity_scores(self, papers: List[Paper]) -> Dict:\n",
        "\n",
        "       ## Calculate similarity scores between papers with improved text processing and error handling.\n",
        "\n",
        "        try:\n",
        "            # Preprocess texts\n",
        "            texts = []\n",
        "            for paper in papers:\n",
        "                # Combine title, abstract, and content for better similarity calculation\n",
        "                text = f\"{paper.title} {paper.abstract} {paper.content}\"\n",
        "                # Basic text cleaning\n",
        "                text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces\n",
        "                text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
        "                text = text.lower().strip()\n",
        "                texts.append(text)\n",
        "\n",
        "            if not texts:\n",
        "                self.logger.error(\"No valid texts found for similarity calculation\")\n",
        "                return {}\n",
        "\n",
        "            # Check if texts contain only stop words or are empty\n",
        "            non_empty_texts = [text for text in texts if text.strip()]\n",
        "            if not non_empty_texts:\n",
        "                self.logger.warning(\"All texts are empty after preprocessing\")\n",
        "                return self._create_empty_similarity_matrix(papers)\n",
        "\n",
        "            try:\n",
        "                # Calculate TF-IDF\n",
        "                tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
        "\n",
        "                # Check if we have any features\n",
        "                if tfidf_matrix.shape[1] == 0:\n",
        "                    self.logger.warning(\"No features extracted from texts\")\n",
        "                    return self._create_empty_similarity_matrix(papers)\n",
        "\n",
        "                # Calculate cosine similarity\n",
        "                similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "                # Store results\n",
        "                similarity_scores = {}\n",
        "                for i, paper1 in enumerate(papers):\n",
        "                    similarity_scores[paper1.title] = {}\n",
        "                    for j, paper2 in enumerate(papers):\n",
        "                        if i != j:\n",
        "                            similarity_scores[paper1.title][paper2.title] = float(similarity_matrix[i][j])\n",
        "\n",
        "                return similarity_scores\n",
        "\n",
        "            except ValueError as ve:\n",
        "                self.logger.error(f\"Vectorization error: {str(ve)}\")\n",
        "                return self._create_empty_similarity_matrix(papers)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in similarity calculation: {str(e)}\")\n",
        "            return self._create_empty_similarity_matrix(papers)\n",
        "\n",
        "    def _create_empty_similarity_matrix(self, papers: List[Paper]) -> Dict:\n",
        "\n",
        "        #Create an empty similarity matrix when similarity calculation fails.\n",
        "\n",
        "        similarity_scores = {}\n",
        "        for paper1 in papers:\n",
        "            similarity_scores[paper1.title] = {}\n",
        "            for paper2 in papers:\n",
        "                if paper1.title != paper2.title:\n",
        "                    similarity_scores[paper1.title][paper2.title] = 0.0\n",
        "        return similarity_scores\n",
        "\n",
        "    def _cluster_topics(self,papers :List[Paper] ) -> Dict:\n",
        "        # Clustiring topics in article\n",
        "        all_keywords = []\n",
        "        for paper in papers:\n",
        "            if paper.keywords:\n",
        "                all_keywords.extend(paper.keywords)\n",
        "\n",
        "        # Frequence Analysis\n",
        "        keyword_freq = defaultdict(int)\n",
        "        for keyword in all_keywords:\n",
        "            keyword_freq[keyword] += 1\n",
        "\n",
        "        #Clustering Topics\n",
        "        clusters = defaultdict(list)\n",
        "        for paper in papers:\n",
        "            if paper.keywords:\n",
        "                main_keyword = max(paper.keywords , key = lambda k : keyword_freq[k])\n",
        "                clusters[main_keyword].append(paper.title)\n",
        "\n",
        "        return dict(clusters)\n",
        "\n",
        "    def _analyze_citation_network(self, papers: List[Paper]) -> Dict:\n",
        "\n",
        "        #Analyze citation network with improved error handling and correct graph metrics calculation.\n",
        "\n",
        "        try:\n",
        "            G = nx.DiGraph()\n",
        "\n",
        "            # Adding Nodes\n",
        "            for paper in papers:\n",
        "                G.add_node(paper.title)\n",
        "\n",
        "            # Adding Edges\n",
        "            for paper in papers:\n",
        "                if paper.references:\n",
        "                    for ref in paper.references:\n",
        "                        for other_paper in papers:\n",
        "                            # More robust reference matching\n",
        "                            if (ref.lower() in other_paper.title.lower() or\n",
        "                                other_paper.title.lower() in ref.lower()):\n",
        "                                G.add_edge(paper.title, other_paper.title)\n",
        "\n",
        "            # Calculate network metrics\n",
        "            metrics = {\n",
        "                'centrality': nx.degree_centrality(G),\n",
        "                'pagerank': nx.pagerank(G),\n",
        "                'node_count': G.number_of_nodes(),\n",
        "                'edge_count': G.number_of_edges()\n",
        "            }\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in citation network analysis: {str(e)}\")\n",
        "            # Return empty metrics if analysis fails\n",
        "            return {\n",
        "                'centrality': {},\n",
        "                'pagerank': {},\n",
        "                'node_count': 0,\n",
        "                'edge_count': 0\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "    def _analyze_methodologies(self, papers: List[Paper]) -> Dict:\n",
        "\n",
        "        methodologies = defaultdict(int)\n",
        "        datasets = defaultdict(int)\n",
        "\n",
        "        # Expanded keyword patterns\n",
        "        methodology_patterns = [\n",
        "        r'(?:method|approach|technique|algorithm|methodology)s?\\s*(?::|is|are|was|were)\\s*([^.]*)',\n",
        "        r'(?:we|authors)\\s+(?:use|used|employ|employed|apply|applied)\\s+([^.]*)',\n",
        "        r'(?:proposed|developed|implemented)\\s+(?:approach|method|technique)\\s+([^.]*)'\n",
        "        ]\n",
        "\n",
        "        dataset_patterns = [\n",
        "        r'(?:dataset|database|corpus|data\\s+set)s?\\s*(?::|is|are|was|were)\\s*([^.]*)',\n",
        "        r'(?:data|samples)\\s+(?:were|was|is|are)\\s+(?:collected|gathered|obtained)\\s+(?:from)?\\s*([^.]*)',\n",
        "        r'(?:we|authors)\\s+(?:use|used|collect|collected)\\s+data\\s+(?:from)?\\s*([^.]*)'\n",
        "        ]\n",
        "\n",
        "        for paper in papers:\n",
        "            # Find methodologies\n",
        "            for pattern in methodology_patterns:\n",
        "                matches = re.finditer(pattern, paper.content, re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    if match.group(1).strip():\n",
        "                        methodologies[match.group(1).strip()] += 1\n",
        "\n",
        "            # Find datasets\n",
        "            for pattern in dataset_patterns:\n",
        "                matches = re.finditer(pattern, paper.content, re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    if match.group(1).strip():\n",
        "                        datasets[match.group(1).strip()] += 1\n",
        "\n",
        "        return {\n",
        "        'methodologies': dict(methodologies),\n",
        "        'datasets': dict(datasets)\n",
        "        }\n",
        "\n",
        "\n",
        "    def generate_report(self,analysis : Analysis , output_dir : str = \"./reports\"):\n",
        "\n",
        "        output_dir = Path(output_dir)\n",
        "        output_dir.mkdir(exist_ok = True)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        report_file = output_dir / f\"research_analysis_{timestamp}.md\"\n",
        "\n",
        "        # Markdown report create\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"# Research Analysis Report\\n\\n\")\n",
        "\n",
        "            f.write(\"## Token Statistics\\n\\n\")\n",
        "            f.write(f\"Total tokens processed: {analysis.total_tokens:,}\\n\\n\")\n",
        "            f.write(\"### Tokens per Paper\\n\")\n",
        "            for title, count in analysis.token_counts.items():\n",
        "                f.write(f\"- {title}: {count:,} tokens\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # BConnections\n",
        "            f.write(\"## Paper Connections\\n\\n\")\n",
        "            for conn in analysis.connections:\n",
        "                f.write(f\"- **Papers**: {' & '.join(conn['papers'])}\\n\")\n",
        "                f.write(f\"  - {conn['description']}\\n\")\n",
        "                f.write(f\"  - Strength: {conn['strength']:.2f}\\n\\n\")\n",
        "\n",
        "            # Research gaps\n",
        "            f.write(\"## Research Gaps\\n\\n\")\n",
        "            for gap in analysis.research_gaps:\n",
        "                f.write(f\"- {gap}\\n\")\n",
        "\n",
        "            # Future directions\n",
        "            f.write(\"\\n## Future Research Directions\\n\\n\")\n",
        "            for direction in analysis.future_directions:\n",
        "                f.write(f\"- {direction}\\n\")\n",
        "\n",
        "            f.write(\"\\n## Key Findings\\n\\n\")\n",
        "            for finding in analysis.key_findings:\n",
        "                f.write(f\"- {finding}\\n\")\n",
        "\n",
        "            # SÄ°milarity Scores\n",
        "            f.write(\"\\n## Paper Similarities\\n\\n\")\n",
        "            f.write(\"| Paper 1 | Paper 2 | Similarity Score |\\n\")\n",
        "            f.write(\"|---------|----------|------------------|\\n\")\n",
        "            for paper1, scores in analysis.similarity_scores.items():\n",
        "                for paper2, score in scores.items():\n",
        "                    f.write(f\"| {paper1} | {paper2} | {score:.3f} |\\n\")\n",
        "\n",
        "            # Topic clusters\n",
        "            f.write(\"\\n## Topic Clusters\\n\\n\")\n",
        "            for topic, papers in analysis.topic_clusters.items():\n",
        "                f.write(f\"### {topic}\\n\")\n",
        "                for paper in papers:\n",
        "                    f.write(f\"- {paper}\\n\")\n",
        "\n",
        "            # Metodoloji analyss\n",
        "            f.write(\"\\n## Methodology Analysis\\n\\n\")\n",
        "            f.write(\"### Common Methodologies\\n\")\n",
        "            for method, count in analysis.methodology_analysis['methodologies'].items():\n",
        "                f.write(f\"- {method}: {count} occurrences\\n\")\n",
        "\n",
        "            f.write(\"\\n### Datasets Used\\n\")\n",
        "            for dataset, count in analysis.methodology_analysis['datasets'].items():\n",
        "                f.write(f\"- {dataset}: {count} occurrences\\n\")\n",
        "\n",
        "            # Citation network metrics\n",
        "            f.write(\"\\n## Citation Network Analysis\\n\\n\")\n",
        "            f.write(\"### Network Metrics\\n\")\n",
        "            f.write(f\"- Number of nodes: {analysis.citation_network['node_count']}\\n\")\n",
        "            f.write(f\"- Number of edges: {analysis.citation_network['edge_count']}\\n\")\n",
        "\n",
        "            f.write(\"\\n### PageRank Scores\\n\")\n",
        "            for paper, score in analysis.citation_network['pagerank'].items():\n",
        "                f.write(f\"- {paper}: {score:.3f}\\n\")\n",
        "\n",
        "            f.write(\"\\n### Centrality Scores\\n\")\n",
        "            for paper, score in analysis.citation_network['centrality'].items():\n",
        "                f.write(f\"- {paper}: {score:.3f}\\n\")\n",
        "\n",
        "        # Create visualizations\n",
        "        self._generate_visualizations(analysis, output_dir, timestamp)\n",
        "\n",
        "        self.logger.info(f\"Report generated successfully: {report_file}\")\n",
        "        return report_file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _generate_visualizations(self, analysis: Analysis, output_dir: Path, timestamp: str):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        - analysis: Analysis object containing all analysis results\n",
        "        - output_dir: Path to output directory\n",
        "        - timestamp: Timestamp string for unique filenames\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # visualizations subdirectory\n",
        "            vis_dir = output_dir / 'visualizations'\n",
        "            vis_dir.mkdir(exist_ok=True)\n",
        "\n",
        "            # Common figure settings for larger visualizations\n",
        "            plt.rcParams.update({\n",
        "                'font.size': 8,  # Smaller base font size\n",
        "                'figure.figsize': (16, 12),  # Larger default figure size\n",
        "                'figure.dpi': 300  # Higher resolution\n",
        "            })\n",
        "\n",
        "            # 1. Similarity Heatmap\n",
        "            similarity_matrix = []\n",
        "            paper_titles = list(analysis.similarity_scores.keys())\n",
        "\n",
        "            # Group papers by topic for better organization\n",
        "            topic_groups = defaultdict(list)\n",
        "            for paper in paper_titles:\n",
        "                topic = next((t for t, papers in analysis.topic_clusters.items()\n",
        "                             if paper in papers), 'Other')\n",
        "                topic_groups[topic].append(paper)\n",
        "\n",
        "            # Sort papers by topic\n",
        "            sorted_papers = []\n",
        "            for topic in sorted(topic_groups.keys()):\n",
        "                sorted_papers.extend(sorted(topic_groups[topic]))\n",
        "\n",
        "            for paper1 in sorted_papers:\n",
        "                row = []\n",
        "                for paper2 in sorted_papers:\n",
        "                    if paper1 == paper2:\n",
        "                        row.append(1.0)\n",
        "                    else:\n",
        "                        row.append(analysis.similarity_scores[paper1].get(paper2, 0))\n",
        "                similarity_matrix.append(row)\n",
        "\n",
        "            plt.figure(figsize=(20, 16))\n",
        "            sns.heatmap(similarity_matrix,\n",
        "                        xticklabels=sorted_papers,\n",
        "                        yticklabels=sorted_papers,\n",
        "                        cmap='YlOrRd',\n",
        "                        annot = True,\n",
        "                        square=True)\n",
        "            plt.xticks(rotation=45, ha='right', fontsize=6)\n",
        "            plt.yticks(fontsize=6)\n",
        "            plt.title('Paper Similarity Heatmap (Grouped by Topic)', pad=20)\n",
        "\n",
        "            # Add topic separators and labels\n",
        "            current_idx = 0\n",
        "            for topic, papers in topic_groups.items():\n",
        "                if current_idx > 0:\n",
        "                    plt.axhline(y=current_idx, color='white', linewidth=2)\n",
        "                    plt.axvline(x=current_idx, color='white', linewidth=2)\n",
        "                current_idx += len(papers)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(vis_dir / f'similarity_heatmap_{timestamp}.png',\n",
        "                        bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # 2. Enhanced Citation Network Graph\n",
        "            G = nx.DiGraph()\n",
        "\n",
        "            # Filter edges based on configurable threshold\n",
        "            similarity_threshold = 0.2 #0.3  # Can be made configurable\n",
        "            for paper1, scores in analysis.similarity_scores.items():\n",
        "                G.add_node(paper1)\n",
        "                for paper2, score in scores.items():\n",
        "                    if score > similarity_threshold:\n",
        "                        G.add_node(paper2)\n",
        "                        G.add_edge(paper1, paper2, weight=score)\n",
        "\n",
        "            #   interactive network visualization using plotly\n",
        "            pos = nx.spring_layout(G, k=1/np.sqrt(len(G.nodes())), iterations=50)\n",
        "\n",
        "            edge_trace = go.Scatter(\n",
        "                x=[], y=[], line=dict(width=2, color='#888'),\n",
        "                hoverinfo='none', mode='lines')\n",
        "\n",
        "            node_trace = go.Scatter(\n",
        "                x=[], y=[], text=[], mode='markers+text',\n",
        "                hoverinfo='text', textposition='bottom center',\n",
        "                marker=dict(\n",
        "                    showscale=True,\n",
        "                    colorscale='YlGnBu',\n",
        "                    size=20,\n",
        "                    colorbar=dict(\n",
        "                        thickness=15,\n",
        "                        title='Node Connections',\n",
        "                        xanchor='left',\n",
        "                        titleside='right'\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Add edges to trace\n",
        "            for edge in G.edges():\n",
        "                x0, y0 = pos[edge[0]]\n",
        "                x1, y1 = pos[edge[1]]\n",
        "                edge_trace['x'] += tuple([x0, x1, None])\n",
        "                edge_trace['y'] += tuple([y0, y1, None])\n",
        "\n",
        "            # Add nodes to trace\n",
        "            for node in G.nodes():\n",
        "                x, y = pos[node]\n",
        "                node_trace['x'] += tuple([x])\n",
        "                node_trace['y'] += tuple([y])\n",
        "                node_trace['text'] += tuple([node])\n",
        "\n",
        "            # Create figure\n",
        "            fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                         layout=go.Layout(\n",
        "                             title='Interactive Paper Citation Network',\n",
        "                             showlegend=False,\n",
        "                             hovermode='closest',\n",
        "                             margin=dict(b=0, l=0, r=0, t=40),\n",
        "                             annotations=[dict(\n",
        "                                 text=\"\",\n",
        "                                 showarrow=False,\n",
        "                                 xref=\"paper\", yref=\"paper\"\n",
        "                             )],\n",
        "                             xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                             yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
        "                         ))\n",
        "\n",
        "            fig.write_html(vis_dir / f'interactive_citation_network_{timestamp}.html')\n",
        "\n",
        "            # 3. Topic Distribution\n",
        "            topic_counts = {topic: len(papers) for topic, papers in analysis.topic_clusters.items()}\n",
        "\n",
        "            # Sort topics by count for better visualization\n",
        "            sorted_topics = dict(sorted(topic_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "            plt.figure(figsize=(16, 8))\n",
        "            bars = plt.bar(range(len(sorted_topics)), sorted_topics.values())\n",
        "            plt.xticks(range(len(sorted_topics)), sorted_topics.keys(),\n",
        "                       rotation=45, ha='right', fontsize=8)\n",
        "\n",
        "            # Add value labels on top of bars\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                        f'{int(height)}',\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "            plt.title('Distribution of Papers Across Topics')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(vis_dir / f'topic_distribution_{timestamp}.png')\n",
        "            plt.close()\n",
        "\n",
        "            # 4. Methodology Pie Chart with Grouping\n",
        "            methodology_counts = analysis.methodology_analysis['methodologies']\n",
        "            if methodology_counts:\n",
        "                # Group small slices into \"Other\" category\n",
        "                threshold = 0.05  # 5% threshold for grouping\n",
        "                total = sum(methodology_counts.values())\n",
        "                grouped_methods = {}\n",
        "                other_sum = 0\n",
        "\n",
        "                for method, count in methodology_counts.items():\n",
        "                    if count/total < threshold:\n",
        "                        other_sum += count\n",
        "                    else:\n",
        "                        grouped_methods[method] = count\n",
        "\n",
        "                if other_sum > 0:\n",
        "                    grouped_methods['Other'] = other_sum\n",
        "\n",
        "                plt.figure(figsize=(20, 16))\n",
        "                plt.pie(grouped_methods.values(),\n",
        "                        labels=grouped_methods.keys(),\n",
        "                        autopct='%1.1f%%',\n",
        "                        textprops={'fontsize': 6})\n",
        "                plt.title('Methodology Distribution (Methods <5% Grouped as Other)')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(vis_dir / f'methodology_distribution_{timestamp}.png')\n",
        "                plt.close()\n",
        "\n",
        "            self.logger.info(f\"Enhanced visualizations generated successfully in {vis_dir}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating visualizations: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def batch_process_papers(self, directory: str) -> List[Paper]:\n",
        "\n",
        "        #Process all PDFs in a directory with improved extraction\n",
        "\n",
        "        papers = []\n",
        "        directory_path = Path(directory)\n",
        "\n",
        "        for pdf_file in directory_path.glob(\"*.pdf\"):\n",
        "            try:\n",
        "                paper = self.parse_paper(str(pdf_file))\n",
        "                if paper:\n",
        "                    # Extract keywords if not already present\n",
        "                    if not paper.keywords:\n",
        "                        paper.keywords = self.extract_keywords(paper.content)\n",
        "\n",
        "                    # Extract references if not already present\n",
        "                    if not paper.references:\n",
        "                        paper.references = self.extract_references(paper.content)\n",
        "\n",
        "                    papers.append(paper)\n",
        "                    self.logger.info(f\"Successfully processed {pdf_file}\")\n",
        "                else:\n",
        "                    self.logger.warning(f\"Could not parse {pdf_file}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error processing {pdf_file}: {str(e)}\")\n",
        "\n",
        "        return papers\n",
        "\n",
        "    def save_analysis(self, analysis: Analysis, filepath: str):\n",
        "\n",
        "        #JSON size record resulting from analysis\n",
        "\n",
        "        try:\n",
        "            # Analysis convert to dict\n",
        "            analysis_dict = {\n",
        "                'connections': analysis.connections,\n",
        "                'research_gaps': analysis.research_gaps,\n",
        "                'future_directions': analysis.future_directions,\n",
        "                'key_findings': analysis.key_findings,\n",
        "                'methodology_analysis': analysis.methodology_analysis,\n",
        "                'topic_clusters': analysis.topic_clusters,\n",
        "                'citation_network': analysis.citation_network,\n",
        "                'similarity_scores': analysis.similarity_scores\n",
        "            }\n",
        "\n",
        "            # Save the json\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(analysis_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            self.logger.info(f\"Analysis saved successfully to {filepath}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving analysis: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def load_analysis(self, filepath: str) -> Analysis:\n",
        "        # Loading saved analysis results\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                analysis_dict = json.load(f)\n",
        "\n",
        "            analysis = Analysis(\n",
        "                connections=analysis_dict['connections'],\n",
        "                research_gaps=analysis_dict['research_gaps'],\n",
        "                future_directions=analysis_dict['future_directions'],\n",
        "                key_findings=analysis_dict['key_findings'],\n",
        "                methodology_analysis=analysis_dict['methodology_analysis'],\n",
        "                topic_clusters=analysis_dict['topic_clusters'],\n",
        "                citation_network=analysis_dict['citation_network'],\n",
        "                similarity_scores=analysis_dict['similarity_scores']\n",
        "            )\n",
        "\n",
        "            self.logger.info(f\"Analysis loaded successfully from {filepath}\")\n",
        "            return analysis\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading analysis: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T18:20:21.705524Z",
          "iopub.execute_input": "2024-11-14T18:20:21.706026Z",
          "iopub.status.idle": "2024-11-14T18:20:21.861443Z",
          "shell.execute_reply.started": "2024-11-14T18:20:21.70598Z",
          "shell.execute_reply": "2024-11-14T18:20:21.860401Z"
        },
        "id": "eqi4cTV2_zOl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# #. Example of using Research Assistant\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "# 1. Starting Research Assistant\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
        "\n",
        "assistant = ResearchAssistant(\n",
        "    api_key=api_key,\n",
        "    cache_dir=\"./cache\",\n",
        "\n",
        ")\n",
        "\n",
        "# 2. Batch processing PDF files\n",
        "pdf_directory = \"/kaggle/input/pdf-files/pdf\"  # PDF'lerin bulunduÄŸu dizin\n",
        "papers = assistant.batch_process_papers(pdf_directory)\n",
        "\n",
        "# 3. Analysis articles\n",
        "analysis = assistant.analyze_papers(papers)\n",
        "\n",
        "#4. Create Report\n",
        "report_path = assistant.generate_report(\n",
        "    analysis=analysis,\n",
        "    output_dir=\"/kaggle/working/reports\"  # RaporlarÄ±n kaydedileceÄŸi dizin\n",
        ")\n",
        "\n",
        "# 6. Save Analysis\n",
        "assistant.save_analysis(\n",
        "    analysis=analysis,\n",
        "    filepath=\"/kaggle/working/analysis_results.json\"\n",
        ")\n",
        "\n",
        "# # # 7.Load Saved Analysis\n",
        "# # loaded_analysis = assistant.load_analysis(\"./analysis_results.json\")\n",
        "\n",
        "# # Example of processing a single PDF file\n",
        "# single_paper = assistant.parse_paper(\"/kaggle/input/example-article/1751-0473-7-7.pdf\")\n",
        "# if single_paper:\n",
        "#     print(f\"Title: {single_paper.title}\")\n",
        "#     print(f\"Authors: {','.join(single_paper.authors)}\")\n",
        "#     print(f\"Year: {single_paper.year}\")\n",
        "#     print(f\"Keywords: {', '.join(single_paper.keywords)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T18:20:21.862733Z",
          "iopub.execute_input": "2024-11-14T18:20:21.863117Z",
          "iopub.status.idle": "2024-11-14T18:21:13.57308Z",
          "shell.execute_reply.started": "2024-11-14T18:20:21.863075Z",
          "shell.execute_reply": "2024-11-14T18:21:13.572028Z"
        },
        "id": "_b-qFxCv_zOs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total tokens processed: {analysis.total_tokens}\")\n",
        "for title, count in analysis.token_counts.items():\n",
        "    print(f\"{title}: {count} tokens\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T18:21:13.574457Z",
          "iopub.execute_input": "2024-11-14T18:21:13.574873Z",
          "iopub.status.idle": "2024-11-14T18:21:13.581424Z",
          "shell.execute_reply.started": "2024-11-14T18:21:13.574829Z",
          "shell.execute_reply": "2024-11-14T18:21:13.580309Z"
        },
        "id": "FMyCFLHO_zOu"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}