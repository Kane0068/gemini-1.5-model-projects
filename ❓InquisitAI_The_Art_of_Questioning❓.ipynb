{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 83735,
          "databundleVersionId": 9881586,
          "sourceType": "competition"
        },
        {
          "sourceId": 10069500,
          "sourceType": "datasetVersion",
          "datasetId": 6206296
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "f96zKEjlAu_d"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "gemini_long_context_path = kagglehub.competition_download('gemini-long-context')\n",
        "kane0068_ai_etic_articles_path = kagglehub.dataset_download('kane0068/ai-etic-articles')\n",
        "google_gemini_1_5_flash_api_api_gemini_1_5_flash_1_path = kagglehub.model_download('google/gemini-1.5-flash-api/Api/gemini-1.5-flash/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "V8xzu2PKAu_h"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❓ InquisitAI - The Art of Questioning❓\n",
        "\n",
        "## 🌟\"What's the foundation of thinking?\n",
        "It's asking questions. From Einstein to Socrates, history's greatest thinkers all started by asking questions. Today, I'm introducing InquisitAI, a system built on this fundamental principle.\"\n",
        "\n",
        "## 🤖 Introduction\n",
        "- InquisitAI is an AI-powered document analysis system\n",
        "- Leveraging the power of Gemini AI, the system doesn't just read documents - it generates intelligent questions about them\n",
        "- Each question learns from the previous answer, diving deeper into the content\n",
        "- The system begins by processing PDF and TXT files, continuously questioning and exploring\n",
        "\n",
        "\n",
        " ## 🧠 Knowledge and Questioning Process\n",
        "\n",
        "High-quality questioning requires deep knowledge. It demands the insight and experience that an expert gains over years of study. InquisitAI, powered by the Gemini model:\n",
        "- Analyzes hundreds of pages in seconds\n",
        "- Identifies relationships between texts\n",
        "- Catches subtle details in research methodology\n",
        "- Most importantly, generates critical questions like an expert\n",
        "\n",
        "Through these questions and answers, users:\n",
        "- Discover research blind spots\n",
        "- Identify new research opportunities\n",
        "- Detect methodological weaknesses\n",
        "- Enhance their own thinking processes\n",
        "\n",
        "\n",
        "## 🔄 Question Generation Mechanism\n",
        "\n",
        "How does the system generate questions?\n",
        "\n",
        "- 🔍 Gap identification\n",
        "- 📊 Methodological concern detection\n",
        "- ⚡ Contradiction discovery\n",
        "- 🎯 Unexplored impact analysis\n",
        "- ✅ Validity assessment of conclusions\n",
        "\n",
        "## 💡 Potential Use Cases\n",
        "\n",
        "### 📚 Academic Research\n",
        "- Literature review\n",
        "- Methodology assessment\n",
        "- Research gap identification\n",
        "\n",
        "### 💼 Business World\n",
        "- Market research analysis\n",
        "- Competitive analysis\n",
        "- Strategy document evaluation\n",
        "\n",
        "### 🎓 Education\n",
        "- Student work assessment\n",
        "- Course material analysis\n",
        "- Critical thinking skills development\n",
        "\n",
        "## 🤝 AI and Human Collaboration\n",
        "\n",
        "InquisitAI exemplifies how artificial intelligence complements rather than competes with humans. The system expands human thinking capacity, accelerates research processes, and offers new perspectives. This allows researchers, students, and professionals to dedicate their time to creative thinking and innovative solutions rather than mechanical analysis.\n",
        "\n",
        "### ✨ Every great discovery, every significant invention, begins with the right question. InquisitAI harnesses the power of artificial intelligence to generate these questions for you, guiding you toward new discoveries. Because we know that without asking the right questions, we cannot reach the right answers. Let InquisitAI keep your questions flowing."
      ],
      "metadata": {
        "id": "XYXgUM2IAu_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Lıbraries\n",
        "\n",
        "import json\n",
        "from typing import List, Dict, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "from datetime import datetime\n",
        "import google.generativeai as genai\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "import pypdf\n",
        "import os\n",
        "from pathlib import Path\n",
        "import mimetypes\n",
        "\n",
        "\n",
        "# Initialize Gemini API\n",
        "user_secrets = UserSecretsClient()\n",
        "api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "@dataclass\n",
        "class TokenUsage:\n",
        "    prompt_tokens: int\n",
        "    completion_tokens: int\n",
        "    total_tokens: int\n",
        "\n",
        "@dataclass\n",
        "class Document:\n",
        "    content: str\n",
        "    file_path: str\n",
        "    file_type: str\n",
        "    timestamp: datetime\n",
        "\n",
        "@dataclass\n",
        "class Question:\n",
        "    content: str\n",
        "    timestamp: datetime\n",
        "    token_usage: Dict\n",
        "\n",
        "@dataclass\n",
        "class Answer:\n",
        "    content: str\n",
        "    timestamp: datetime\n",
        "    token_usage: Dict\n",
        "\n",
        "@dataclass\n",
        "class Analysis:\n",
        "    content: Dict\n",
        "    timestamp: datetime\n",
        "    token_usage: Dict\n",
        "\n",
        "def upload_to_gemini(file_path: str) -> Dict:\n",
        "    \"\"\"Upload file to Gemini with appropriate mime type\"\"\"\n",
        "    mime_type = mimetypes.guess_type(file_path)[0]\n",
        "    if mime_type is None:\n",
        "        # Default to plain text if mime type cannot be determined\n",
        "        mime_type = 'text/plain'\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        file_content = f.read()\n",
        "\n",
        "    return {\n",
        "        \"mime_type\": mime_type,\n",
        "        \"data\": file_content\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InquisitAI:\n",
        "    def __init__(self):\n",
        "        # Initialize Gemini model with configuration\n",
        "        generation_config = {\n",
        "            \"temperature\": 1,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 64,\n",
        "            \"max_output_tokens\": 8192,\n",
        "            \"response_mime_type\": \"text/plain\",\n",
        "        }\n",
        "\n",
        "        self.current_context = \"\"  # To hide the last answer\n",
        "        self.discussion_thread = []  # To follow the flow of discussion\n",
        "\n",
        "\n",
        "        self.model = genai.GenerativeModel(\n",
        "            model_name=\"gemini-1.5-flash-latest\",\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        self.chat_session = self.model.start_chat()\n",
        "\n",
        "        self.documents: List[Document] = []\n",
        "\n",
        "        self.discussion_history = []\n",
        "        self.total_token_usage = TokenUsage(0, 0, 0)\n",
        "\n",
        "\n",
        "\n",
        "    def update_token_count(self, response) -> Dict:\n",
        "        \"\"\"Update and return token usage from response metadata\"\"\"\n",
        "        metadata = response.usage_metadata\n",
        "        token_usage = {\n",
        "            \"prompt_tokens\": metadata.prompt_token_count,\n",
        "            \"completion_tokens\": metadata.candidates_token_count,\n",
        "            \"total_tokens\": metadata.total_token_count\n",
        "        }\n",
        "\n",
        "        self.total_token_usage.prompt_tokens += token_usage[\"prompt_tokens\"]\n",
        "        self.total_token_usage.completion_tokens += token_usage[\"completion_tokens\"]\n",
        "        self.total_token_usage.total_tokens += token_usage[\"total_tokens\"]\n",
        "\n",
        "        print(f\"\\nToken Usage for this response:\")\n",
        "        print(f\"Prompt tokens: {token_usage['prompt_tokens']}\")\n",
        "        print(f\"Completion tokens: {token_usage['completion_tokens']}\")\n",
        "        print(f\"Total tokens: {token_usage['total_tokens']}\")\n",
        "\n",
        "        return token_usage\n",
        "\n",
        "    def load_documents(self, file_paths: List[str]) -> bool:\n",
        "        \"\"\"Load and process multiple documents using Gemini file upload\"\"\"\n",
        "        try:\n",
        "            uploaded_files = []\n",
        "\n",
        "            # Upload each document to Gemini\n",
        "            for file_path in file_paths:\n",
        "                path = Path(file_path)\n",
        "                if not path.exists():\n",
        "                    print(f\"File not found: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\nUploading document: {path.name}\")\n",
        "                uploaded_file = upload_to_gemini(str(path))\n",
        "                uploaded_files.append(uploaded_file)\n",
        "\n",
        "                self.documents.append(Document(\n",
        "                    content=\"\",  # Content will be handled by Gemini\n",
        "                    file_path=str(path),\n",
        "                    file_type=path.suffix.lower()[1:],\n",
        "                    timestamp=datetime.now()\n",
        "                ))\n",
        "\n",
        "            if not uploaded_files:\n",
        "                print(\"Error: No valid documents were uploaded\")\n",
        "                return False\n",
        "\n",
        "            # Initialize chat session with uploaded files\n",
        "            history = [{\n",
        "                \"role\": \"user\",\n",
        "                \"parts\": uploaded_files\n",
        "            }]\n",
        "\n",
        "            self.chat_session = self.model.start_chat(history=history)\n",
        "\n",
        "            # Verify document understandin\n",
        "            verify_prompt = f\"I have uploaded {len(uploaded_files)} documents for analysis. Please confirm you can access and understand their content.\"\n",
        "            response = self.chat_session.send_message(verify_prompt)\n",
        "            token_usage = self.update_token_count(response)\n",
        "\n",
        "            if \"confirm\" in response.text.lower() or \"understand\" in response.text.lower():\n",
        "                print(f\"\\nDocuments successfully loaded and processed:\")\n",
        "                for doc in self.documents:\n",
        "                    print(f\"- {Path(doc.file_path).name}\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"Error: Model did not confirm document understanding\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in document loading: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def generate_question(self) -> Question:\n",
        "        \"\"\"Generate a single focused question about the documents\"\"\"\n",
        "        if not self.documents:\n",
        "            return Question(\n",
        "                content=\"Error: No documents loaded\",\n",
        "                timestamp=datetime.now(),\n",
        "                token_usage={\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0}\n",
        "            )\n",
        "\n",
        "        if not self.current_context:\n",
        "            prompt = f\"\"\"Based on the {len(self.documents)} documents provided earlier,\n",
        "            generate ONE specific, focused question that critically examines:\n",
        "            - A potential gap or limitation in the research\n",
        "            - A methodological concern\n",
        "            - An unexplored implication\n",
        "            - A conflicting finding\n",
        "            - The validity of a specific conclusion\n",
        "\n",
        "            Requirements:\n",
        "            1. Generate ONLY ONE question\n",
        "            2. The question must be specific and focused on a single issue\n",
        "            3. Avoid broad, multi-part questions\n",
        "            4. Focus on concrete rather than abstract concerns\n",
        "\n",
        "            Format: Return only the single question, without any preamble or additional text.\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"Based on the previous answer: \"{self.current_context}\"\n",
        "\n",
        "            If the answer was inadequate or raised new issues, generate ONE specific follow-up question that:\n",
        "            1. Addresses any unclear or unsupported claims in the previous answer\n",
        "            2. Probes deeper into a specific point made\n",
        "            3. Challenges a specific assumption\n",
        "\n",
        "            Requirements:\n",
        "            1. Generate ONLY ONE focused question\n",
        "            2. The question must directly relate to the previous answer\n",
        "            3. If the answer was fully satisfactory, respond with exactly \"SATISFIED\"\n",
        "\n",
        "            Format: Return only the single question or \"SATISFIED\", without any other text.\"\"\"\n",
        "\n",
        "        response = self.chat_session.send_message(prompt)\n",
        "        token_usage = self.update_token_count(response)\n",
        "\n",
        "        # Check if the response contains multiple questions (looking for number prefixes or multiple question marks)\n",
        "        response_text = response.text.strip()\n",
        "        if response_text.count('?') > 1 or any(str(i)+'.' in response_text for i in range(1,10)):\n",
        "            # If multiple questions detected, take only the first complete question\n",
        "            questions = [q.strip() for q in response_text.split('?') if q.strip()]\n",
        "            response_text = questions[0] + '?'\n",
        "\n",
        "        return Question(\n",
        "            content=response_text,\n",
        "            timestamp=datetime.now(),\n",
        "            token_usage=token_usage\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_answer(self, question: str) -> Answer:\n",
        "        \"\"\"Get a specific and detailed answer from the model\"\"\"\n",
        "        prompt = f\"\"\"Provide a specific, detailed answer to this question:\n",
        "        {question}\n",
        "\n",
        "        Requirements:\n",
        "        1. Address the question directly and specifically\n",
        "        2. Provide concrete examples or evidence where possible\n",
        "        3. Acknowledge any limitations or uncertainties\n",
        "        4. Stay focused on the specific question asked\n",
        "\n",
        "        Format: Provide a clear, structured response that directly addresses the question.\"\"\"\n",
        "\n",
        "        response = self.chat_session.send_message(prompt)\n",
        "        token_usage = self.update_token_count(response)\n",
        "\n",
        "        return Answer(\n",
        "            content=response.text.strip(),\n",
        "            timestamp=datetime.now(),\n",
        "            token_usage=token_usage\n",
        "        )\n",
        "\n",
        "    def analyze_answer(self, question: str, answer: str) -> Analysis:\n",
        "        \"\"\"Analyze the quality and completeness of an answer\"\"\"\n",
        "        analysis_prompt = f\"\"\"Analyze this Q&A concisely:\n",
        "        Question: {question}\n",
        "        Answer: {answer}\n",
        "\n",
        "        Provide a structured analysis with scores (1-10) for relevance, depth, and clarity.\n",
        "        Format your response EXACTLY like this example:\n",
        "        {{\n",
        "            \"scores\": {{\n",
        "                \"relevance\": 8,\n",
        "                \"depth\": 7,\n",
        "                \"clarity\": 9\n",
        "            }},\n",
        "            \"needs_followup\": true,\n",
        "            \"recommendations\": [\"Be more specific\", \"Add examples\"]\n",
        "        }}\"\"\"\n",
        "\n",
        "        response = self.chat_session.send_message(analysis_prompt)\n",
        "        token_usage = self.update_token_count(response)\n",
        "\n",
        "        try:\n",
        "            # Clean the response text to ensure valid JSON\n",
        "            cleaned_response = response.text.strip()\n",
        "            # Remove any markdown formatting if present\n",
        "            if cleaned_response.startswith(\"```json\"):\n",
        "                cleaned_response = cleaned_response[7:-3]\n",
        "            elif cleaned_response.startswith(\"```\"):\n",
        "                cleaned_response = cleaned_response[3:-3]\n",
        "\n",
        "            analysis_content = json.loads(cleaned_response)\n",
        "\n",
        "            # Validate expected structure\n",
        "            required_keys = {\"scores\", \"needs_followup\", \"recommendations\"}\n",
        "            if not all(key in analysis_content for key in required_keys):\n",
        "                raise ValueError(\"Missing required keys in analysis\")\n",
        "\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            print(f\"Analysis parsing error: {str(e)}\")\n",
        "            analysis_content = {\n",
        "                \"scores\": {\"relevance\": 5, \"depth\": 5, \"clarity\": 5},\n",
        "                \"needs_followup\": True,\n",
        "                \"recommendations\": [\"Error parsing analysis - using default values\"]\n",
        "            }\n",
        "\n",
        "        return Analysis(\n",
        "            content=analysis_content,\n",
        "            timestamp=datetime.now(),\n",
        "            token_usage=token_usage\n",
        "        )\n",
        "\n",
        "    def run_discussion(self, max_turns: int = 5) -> List[Dict]:\n",
        "        \"\"\"Run an interactive discussion about the documents with optimized token usage\"\"\"\n",
        "        discussion = []\n",
        "        turn = 0\n",
        "\n",
        "        print(\"\\n=== Starting Document Analysis Discussion ===\")\n",
        "\n",
        "        while turn < max_turns:\n",
        "            print(f\"\\n--- Turn {turn + 1}/{max_turns} ---\")\n",
        "\n",
        "            # Generate question with reduced context\n",
        "            question = self.generate_question()\n",
        "            if question.content == \"SATISFIED\":\n",
        "                print(\"\\nDiscussion complete - All points adequately addressed\")\n",
        "                break\n",
        "\n",
        "            print(f\"\\nQuestion: {question.content}\")\n",
        "\n",
        "            # Get answer with simplified prompt\n",
        "            answer = self.get_answer(question.content)\n",
        "            print(f\"\\nAnswer: {answer.content}\")\n",
        "\n",
        "            # Store minimal context for next question\n",
        "            self.current_context = answer.content[:500]  # Limit context size\n",
        "\n",
        "            # Analyze response with structured format\n",
        "            analysis = self.analyze_answer(question.content, answer.content)\n",
        "            print(\"\\nAnalysis:\", json.dumps(analysis.content, indent=2))\n",
        "\n",
        "            # Record discussion entry with optimized structure\n",
        "            entry = {\n",
        "                \"turn\": turn + 1,\n",
        "                \"question\": question.content,\n",
        "                \"answer\": answer.content,\n",
        "                \"analysis\": analysis.content,\n",
        "                \"token_usage\": {\n",
        "                    \"total\": (\n",
        "                        question.token_usage[\"total_tokens\"] +\n",
        "                        answer.token_usage[\"total_tokens\"] +\n",
        "                        analysis.token_usage[\"total_tokens\"]\n",
        "                    )\n",
        "                }\n",
        "            }\n",
        "\n",
        "            discussion.append(entry)\n",
        "            self.discussion_thread.append(entry)\n",
        "\n",
        "            print(f\"\\nToken usage this turn: {entry['token_usage']['total']}\")\n",
        "\n",
        "            turn += 1\n",
        "\n",
        "        return discussion\n",
        "\n",
        "    def export_results(self, filepath: str) -> bool:\n",
        "        \"\"\"Export complete discussion and analysis results\"\"\"\n",
        "        try:\n",
        "            output = {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"document_count\": len(self.documents),\n",
        "                    \"document_types\": [doc.file_type for doc in self.documents]\n",
        "                },\n",
        "                \"token_usage\": {\n",
        "                    \"prompt_tokens\": self.total_token_usage.prompt_tokens,\n",
        "                    \"completion_tokens\": self.total_token_usage.completion_tokens,\n",
        "                    \"total_tokens\": self.total_token_usage.total_tokens\n",
        "                },\n",
        "                \"discussion\": self.discussion_history\n",
        "            }\n",
        "\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(output, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"\\nResults exported to: {filepath}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error exporting results: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "def get_documents_from_folder(folder_path: str) -> List[str]:\n",
        "    documents = []\n",
        "    folder_path = Path(folder_path)\n",
        "\n",
        "    # Check if folder exists\n",
        "    if not folder_path.exists():\n",
        "        print(f\"Folder not found: {folder_path}\")\n",
        "        return documents\n",
        "\n",
        "    # Recursively search for PDF and TXT files\n",
        "    for file_path in folder_path.rglob(\"*\"):\n",
        "        if file_path.is_file() and file_path.suffix.lower() in ['.pdf', '.txt']:\n",
        "            documents.append(str(file_path))\n",
        "            print(f\"Found document: {file_path.name}\")\n",
        "\n",
        "    print(f\"\\nTotal documents found: {len(documents)}\")\n",
        "    return documents\n",
        "\n",
        "def main():\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = Path(\"output\")\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = InquisitAI()\n",
        "\n",
        "    # # Specify documents to analyze\n",
        "    # documents = [\n",
        "    #     \"path/to/your/document1.pdf\",\n",
        "    #     \"path/to/your/document2.txt\"\n",
        "    # ]\n",
        "\n",
        "    # Get all documents from the specified folder\n",
        "    documents_folder = \"/kaggle/input/ai-etic-articles\"  # Change this to your folder path\n",
        "    documents = get_documents_from_folder(documents_folder)\n",
        "\n",
        "    if not documents:\n",
        "        print(\"No PDF or TXT documents found in the specified folder\")\n",
        "        return\n",
        "\n",
        "    # Run analysis\n",
        "    if analyzer.load_documents(documents):\n",
        "        discussion = analyzer.run_discussion(max_turns=5)\n",
        "        analyzer.discussion_history = discussion\n",
        "\n",
        "        # Export results\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_file = output_dir / f\"discussion_analysis_{timestamp}.json\"\n",
        "        analyzer.export_results(output_file)\n",
        "\n",
        "        # Print final statistics\n",
        "        print(\"\\n=== Final Statistics ===\")\n",
        "        print(f\"Documents analyzed: {len(analyzer.documents)}\")\n",
        "        print(f\"Discussion turns: {len(discussion)}\")\n",
        "        print(f\"Total tokens used: {analyzer.total_token_usage.total_tokens}\")\n",
        "    else:\n",
        "        print(\"Analysis failed due to document loading error\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T19:24:25.839292Z",
          "iopub.execute_input": "2024-12-01T19:24:25.839868Z",
          "iopub.status.idle": "2024-12-01T19:27:19.777379Z",
          "shell.execute_reply.started": "2024-12-01T19:24:25.839817Z",
          "shell.execute_reply": "2024-12-01T19:27:19.775493Z"
        },
        "id": "-3GXYsROAu_j"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}